<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>豆瓣电影爬虫梳理</title>
      <link href="%E8%B1%86%E7%93%A3%E7%94%B5%E5%BD%B1%E7%88%AC%E8%99%AB%E6%A2%B3%E7%90%86/"/>
      <url>%E8%B1%86%E7%93%A3%E7%94%B5%E5%BD%B1%E7%88%AC%E8%99%AB%E6%A2%B3%E7%90%86/</url>
      
        <content type="html"><![CDATA[<p>❗️爬虫会对网站产生负担和资源浪费，无论所在地区是否合法，过度使用都会引发道德问题。本项目只用于个人学习实践，具体的代码可在 <a href="https://github.com/jakewg/DoubanWebScraper">Github</a> 找到。</p><p>最近看了一些用 Python 实现基础数据结构和算法的书，虽然依旧是云里雾里，但是对基本的代码逻辑和 Python 数据对象的特点更熟悉了些。因此，我索性就实现一下很久以前就想实现的项目：把我过去在豆瓣标记过的电影都下载下来。从去年开始我开始使用私有数据库做电影和图书的标记，主要原因是觉得数据在自己手里，管理和分析起来方便一些。</p><span id="more"></span><h2 id="爬虫的基本工作逻辑"><a href="#爬虫的基本工作逻辑" class="headerlink" title="爬虫的基本工作逻辑"></a>爬虫的基本工作逻辑</h2><p>要解释爬虫的基本工作逻辑，我们可以用网页浏览器做简单的比喻。<br>网页浏览器的工作模式可以简化为三个步骤：</p><ol><li>告诉服务器我们需要访问哪一个网页（输入网址）</li><li>服务器将我们要访问的网页数据发送回来</li><li>浏览器将收到的数据（文字、图片等等）可视化呈现给我们</li></ol><p>那么，网页爬虫也同样适用这样三个步骤：</p><ol><li>使用 Python 请求我们想要获取的网页</li><li>服务器将我们要访问的网页数据发送回来</li><li>使用 Python 解读收到的数据，只提取我们所需要的部分呈现</li></ol><p>浏览器可以帮助我们完成浏览网页所需要的三个步骤，那么相对的 Python 中的函数包可以帮助我们非常方便的完成网页爬虫所需要的三个任务。唯一需要更多工作的是第三部，浏览器根据 html 代码自动呈现内容，而爬虫根据网页的内容，需要使用 Python 手动寻找所需要的数据。</p><h2 id="实现概述"><a href="#实现概述" class="headerlink" title="实现概述"></a>实现概述</h2><p>框架已经搭建好，按照上面的逻辑一步步实现即可。这里只是粗略的叙述，记下有趣的地方，具体的代码参考 Github repo。</p><p>豆瓣是一个开放的社区，只要用户设置了公开，我们可以直接看到对方的主页。这里我需要做的第一步是找到用户主页，把已看过的电影的列表，包含电影名字和对应的豆瓣网址抓取下来。需要处理的第一个点是：豆瓣默认每页只会展示三十条电影记录。因此，我们需要一个“下一页确认器”，这个 checker 可以做出一个判断并根据判断结果执行一条命令。即如果它判断当前页面存在“下一页”按钮，那它就自动获取下一页的网址，并将新的网址作为用户主页再次运行一个循环；如果不存在“下一页”按钮，则输出“已经全部抓取”的提示并终止整个过程。</p><p>那么第一步，对于全部已看过电影名称的抓取，我们可以归纳为：</p><ol><li>使用 Python 函数包 <code>request</code> 下载网页页面；使用 <code>BeautifulSoup</code> 作为 parser 对网页数据进行读取整理。</li><li>从用户已看过电影记录页面的第一页开始，抓取一整页的标记电影名称和链接</li><li>使用 checker 检查是否存在下一页，如果存在下一页则循环重复1 2 3步</li><li>将左右获取的电影名称和电影网址存入文档中</li></ol><p>至此我们已经完成了抓取的第一步。既然已经有了所有的电影网址，那么使用同样的技术下载对应的电影页面后，我们就可从每一个单体电影的页面中找到我们感兴趣的数据。第二步，找每个电影页面中有趣的数据，其逻辑仍旧是不变，但和第一步只有在两点实践层面的不同。即不再需要使用 checker ，且每一页需要抓取的数据位置更为复杂。</p><p>相对于单纯的已看过电影的列表，单个电影页面显然更加有趣。在项目 demo 中，我分别抓取了电影名称、首次发布时间、电影类别、导演、卡司、制片国家和 imdb 网址。这是工作量较大的部分，对于每一个数据点，我们都需要读取对应的 html 代码以确定数据的位置。我使用了 Firefox 浏览器的开发者工具，可帮助我在大量的 html 代码里定位目标数据。</p><h2 id="反爬机制"><a href="#反爬机制" class="headerlink" title="反爬机制"></a>反爬机制</h2><p>豆瓣的网页源码相对简单，没有很多页面效果或 JavaScript，但它同样有反爬机制。但对于学习和研究来说，我们只需简单的方式即可解决，毕竟爬取效率和稳定性在非生产环境中不是特别需要考虑的重点。</p><p>应对反爬的方式都基于一个很容易理解的原理：隐藏我们是自动程序的真实身份，让我们的程序行为看起来来自于真实用户。在现有的例子中，我只使用两种非常简单的方式。第一，通过设定 “User-Agent”，我们试图让网站服务器认为网页的请求来自真实用户。网站服务器依赖 “User-Agent” 中包含的信息，例如浏览器类别、操作系统版本等等，判断用户的特征以回传合适的数据。例如给 iOS 的 Safari 浏览器发送针对手机用户优化的网页。我们设置通过合适的特征，可以让网站服务认为我在使用 iPhone 手机正常浏览网页。 甚至更进一步，我们可以生成不同的的 “User-Agent” 来模拟不同真实用户的网页请求。</p><p>另一个设定是请求间隔。爬虫一个非常明显的特征是速度很快。它可以用远超人类反应的速度发送请求、读取数据，毕竟计算机的反应速度比人手点击鼠标要快得多。因此，过于快速的网页请求很快会暴露出异常。因此，大多时候的反爬机制都可以用较慢速的循环运行规避。在本例子中，我设定了每次循环 6 秒的请求间隔，用一种类似人类打开并滚动页面的速度来隐藏自己的网页请求。</p><p>绕过反爬的方式还有很多，毕竟设置时间间隔会虽然简单，但会使爬虫的效率下降非常多。其他效率更高的方式，例如使用 Cookie 或设置 IP 地址代理池，或可在之后的学习中实现。设置 Cookie 还可以让我们抓取登陆后才可查看的数据，例如我们自身账户内的评分和评价。</p><h2 id="下一步"><a href="#下一步" class="headerlink" title="下一步"></a>下一步</h2><p>正如我提到的，下一步我会尝试使用 Cookie 的方式对我个人的电影主观评分和短评进行抓取和分析。但其中涉及到验证码识别 CAPTCHAs 过程可能是一个难点。之后可以会对我自己的电影偏好进行分析和可视化，或者使用 NLP 和 NN 做一个推荐系统练手……</p>]]></content>
      
      
      
        <tags>
            
            <tag> Tool </tag>
            
            <tag> Python </tag>
            
            <tag> Scraper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GitHub 客户端简明教程</title>
      <link href="Github%20%E5%AE%A2%E6%88%B7%E7%AB%AF%E7%AE%80%E6%98%8E%E6%95%99%E7%A8%8B/"/>
      <url>Github%20%E5%AE%A2%E6%88%B7%E7%AB%AF%E7%AE%80%E6%98%8E%E6%95%99%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<p>这学期的小组作业要写不少代码，一直在寻找共享工作代码的方式，最好还能带有版本控制功能以防有人失手引入错误造成不必要的麻烦。手边最现成的方案是 GitHub，但是要求全组非 IT 背景同学在命令行使用 Git 是一个学习成本过高的事情，遂作罢。直至今天发现 GitHub 的图形界面客户端非常简洁易用，五分钟就能上手，不用配置 Git 不用使用命令行，于是决定写一篇简明教程。</p><span id="more"></span><h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><ol><li>注册<a href="https://github.com/">GitHub</a>账号</li><li>下载<a href="https://desktop.github.com/">GitHub Desktop</a>客户端</li></ol><h2 id="Git-的简单原理"><a href="#Git-的简单原理" class="headerlink" title="Git 的简单原理"></a>Git 的简单原理</h2><p>简单谈一下原理，明白Git 的原理应有助于继续学习使用。只提及最简单且必要的步骤，其他的例如 add 和 brach 等暂不涉及。</p><p>我们可以把整个全过程类比成一个网盘同步的过程。</p><ol><li>网盘：在本地设置一个文件夹，将云端的内容下载到本地，自动。<br> GitHub：在本地设置一个文件，将远端的代码文件下载到本地，手动动作 Clone。</li><li>网盘：在本地修改文件后检测到文件的不同，自动记录。<br> GitHub：在本地修改文件后检测到文件的不同，自动检测到修改，需要手动记录动作 Commit。</li><li>网盘：自动上传修改后文件。<br> GitHub：需要手动动作 Push 将修改的文件和修改记录上传到远端。</li><li>网盘：自动检测到云端文件变化，自动下载到本地。<br> GitHub：手动动作 Fetch / Pull  将远端的变化文件下载到本地。修改后重复第二步和第三步</li></ol><h2 id="GitHub-Desktop-的使用"><a href="#GitHub-Desktop-的使用" class="headerlink" title="GitHub Desktop 的使用"></a>GitHub Desktop 的使用</h2><p>打开 GitHub 客户端后首先登陆账号，并设置用户名和邮箱作为你在项目的身份辨识。<br>推荐使用默认的 Tutorial 示例作为教程，简明易懂。<br><img src="https://raw.githubusercontent.com/jakewg/jw_blog_file_host/master/pages_img/Github_%E5%AE%A2%E6%88%B7%E7%AB%AF%E7%AE%80%E6%98%8E%E6%95%99%E7%A8%8B_1.png"><br>点击从互联网克隆一个仓库，输入想要克隆的仓库地址， 例如 <a href="https://github.com/spyder-ide/spyder.git">https://github.com/spyder-ide/spyder.git</a>。<br><img src="https://raw.githubusercontent.com/jakewg/jw_blog_file_host/master/pages_img/Github_%E5%AE%A2%E6%88%B7%E7%AB%AF%E7%AE%80%E6%98%8E%E6%95%99%E7%A8%8B_2.png"><br>第一次成功克隆之后就可以对代码或数据进行修改和保存，GitHub 客户端能自动检测到修改的内容并以红绿色块高亮出来。在你确定要同步这些修改后，可在 commit 区域添加修改的注释并点击确认。<br><img src="https://raw.githubusercontent.com/jakewg/jw_blog_file_host/master/pages_img/Github_%E5%AE%A2%E6%88%B7%E7%AB%AF%E7%AE%80%E6%98%8E%E6%95%99%E7%A8%8B_3.png"><br>这时便能看到 Push 的提醒，点击后即可把修改过的代码上传到云仓库。需要注意的是在你进行下一次修改代码文件之前，请先点击 Fetch 按钮将云仓库的文件同步到本地，以防止云端文件的改变未同步至本地。<br><img src="https://raw.githubusercontent.com/jakewg/jw_blog_file_host/master/pages_img/Github_%E5%AE%A2%E6%88%B7%E7%AB%AF%E7%AE%80%E6%98%8E%E6%95%99%E7%A8%8B_4.png"></p>]]></content>
      
      
      
        <tags>
            
            <tag> Tool </tag>
            
            <tag> Study </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>关于情绪管理的读书笔记《Undoing Depression》—  Part 1</title>
      <link href="%E5%85%B3%E4%BA%8E%E6%83%85%E7%BB%AA%E7%AE%A1%E7%90%86%E7%9A%84%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E3%80%8AUndoing%20Depression%E3%80%8B%E2%80%94%20%20Part%201/"/>
      <url>%E5%85%B3%E4%BA%8E%E6%83%85%E7%BB%AA%E7%AE%A1%E7%90%86%E7%9A%84%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E3%80%8AUndoing%20Depression%E3%80%8B%E2%80%94%20%20Part%201/</url>
      
        <content type="html"><![CDATA[<p>二月初国内疫情严重时，我做过一个心理健康在线测试，结果只有28/100分。之后我意识到，任何时间阶段的不确定性带来的情绪波动都会给个人生活质量造成巨大影响。对于较为长期的抑郁情绪来说，药物治疗是一个简单快捷的手段，但且不论它对于建立一个良好的心理状况有没有帮助，其可能产生的副作用和依赖性风险，就值得每一个使用者再三斟酌。这片博文观点的基础，《Undoing Depression 走出抑郁》的作者 Richard O’Connor 估算，约有 25% 的人会在人生的某一阶段遭遇中等程度抑郁。这也是意味着，我身边的每四个朋友中就有一个会遇到这样的危机。当下新冠危机仍在继续，希望 Richard O’Connor 医生的理论依据和体验能建立一个系统的情绪管理体系，以帮助处于心理困境的朋友。</p><span id="more"></span><h2 id="理解抑郁"><a href="#理解抑郁" class="headerlink" title="理解抑郁"></a>理解抑郁</h2><p>基于美国官方统计，大约有 6.7% 的美国人在一生中会遇到较为重大的抑郁。抑郁对于经济的影响仅次于癌症，约等于心脏病和艾滋，其包括人力资源的损失，药物与治疗费用。大多数抑郁的人都有同样的心理状态：感到羞耻并不愿承认、会责怪自己太过弱小以至于心理困难都克服不了。不同于普通的疾病，也是抑郁最为残酷的一点：自责和内疚正是其症状但自我意识不到。对于抑郁患者来说，其神经化学物质是不同于常人的，因此它从来不单单是心理问题而是同样包含了身体问题。人们往往不会承认自己的抑郁，但会从生活中的某一个点表现出来，感情、酒精、药物和糟糕的学习与工作。想要认识到它，自我承认其病理性是第一步。</p><h2 id="抑郁的体验"><a href="#抑郁的体验" class="headerlink" title="抑郁的体验"></a>抑郁的体验</h2><p>在普通人看来，抑郁常常和伤心、悲痛混淆。但抑郁的反面不是幸福，而是活力，是感受情感的能力。它可以包括伤心，高兴与兴奋等等。抑郁的标志是长时间的难过或是没有情感，中等抑郁的人仍旧可以吃饭、工作、娱乐与进行性生活，但看上去很空洞；更加严重的人会从一系列的活动中退出并对参与感到精疲力竭。以下使用 O’Connor 医生的观点作为综合评述：</p><ol><li>抑郁的感情技巧<ul><li>情感隔离，即能意识到周遭发生的一切但不能体会到与之相伴的情绪。</li><li>躯体化</li><li>否认与压抑</li></ul></li><li>抑郁的行为技巧<ul><li>拖延</li><li>嗜睡</li><li>不停的工作但不分主次，盲目的前进</li><li>强迫行为，多数与生活中的恐惧相关</li><li>伤害与暴力或是成为受害者</li></ul></li><li>抑郁的认知技巧<ul><li>悲观，期待最坏的结果以此避免失望</li><li>消极的自我对话</li><li>被动，容易被外界的强制力量控制</li><li>选择性注意，即只关注证实了预期的事物</li></ul></li><li>抑郁的人际交往技巧<ul><li>招募同伙，将社交圈子限定在那些对你没有期望的人以内</li><li>社会隔离</li><li>依赖</li><li>反依赖，即表现的仿佛不需要任何人</li></ul></li><li>对自我的抑郁性对待<ul><li>虚高的目标，极低的期待</li><li>没有目标，充满内疚</li><li>对自我的被动攻击</li></ul></li><li>对身体的抑郁性对待<ul><li>疲惫与衰竭的不断循环</li><li>缺乏锻炼</li><li>忽视药物治疗</li><li>预防性进食，用食物来堵塞情绪</li><li>滥用药物和酒精</li></ul></li></ol><h2 id="阐释"><a href="#阐释" class="headerlink" title="阐释"></a>阐释</h2><p>当我们讨论抑郁的时候，是喜忧参半的。让人恐惧的消息是，长时间抑郁会造成脑损伤。人会失去分泌多巴胺的能力，而多巴胺是快乐系统中首要的神经递质。其次，海马体同样会随着重度抑郁的发作而缩小，这也可能解释了抑郁患者为什么在集中注意力以及记忆上存在困难，因为海马承担着将短时记忆转换为长时记忆的重要作用。但让人振奋的好消息是，通过练习，大脑可以被改变和修复。有关冥想的研究显示，有规律的练习能重塑大脑，其可以增强前额皮层的活跃性，而前额皮层则被认为是自我意识产生的地方。研究认为，压力经历会造成神经化学物质的不平衡，长时间的不平衡性会发展成为一种慢性疾病。作为抑郁的一部分，恶性循环也是不得不提的一部分，因为以下状况会互相强化：</p><ul><li>抑郁思维</li><li>自我妨碍行为，例如酒精滥用、拖延、混乱、嗜睡和优柔寡断</li><li>内疚、自责和降低的自尊</li><li>害怕情绪失控</li><li>生活的大部分方面运作不良</li></ul><p>以上为Richard O’Connor 医生的著作《Undoing Depression》的第一部分 “了解抑郁”的主要内容及一些主观理解，略去了一些学术性的分析。接下来的一篇，将包括更有指导意义的第二部分。</p><p>Proof reading: Greg</p>]]></content>
      
      
      
        <tags>
            
            <tag> Books </tag>
            
            <tag> Life </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>高效稳定访问校园网络的方式</title>
      <link href="%E9%AB%98%E6%95%88%E7%A8%B3%E5%AE%9A%E8%AE%BF%E9%97%AE%E6%A0%A1%E5%9B%AD%E7%BD%91%E7%BB%9C%E7%9A%84%E6%96%B9%E5%BC%8F/"/>
      <url>%E9%AB%98%E6%95%88%E7%A8%B3%E5%AE%9A%E8%AE%BF%E9%97%AE%E6%A0%A1%E5%9B%AD%E7%BD%91%E7%BB%9C%E7%9A%84%E6%96%B9%E5%BC%8F/</url>
      
        <content type="html"><![CDATA[<p>这篇文章旨在帮助同学和朋友提升访问校园网络资源的速度和稳定性，以及使用 Google Scholar 或 Wikipedia 及其他学术工具。在使用的同时请遵守当地的法律法规。</p><p>如果你仍旧能访问学校网络，请直接使用学校官网搜索 ‘VPN’ 后遵循学校官方指导。如你访问学校网站时发生困难，如加载超时、部分加载等，请尝试以下方式：</p><span id="more"></span><p>绝大多数院校使用的 VPN 方案为 Cisco 的 AnyConnect 方案，其安装和使用都非常的简单，只需使用到三个变量：</p><ol><li>Connect to, 即你要访问学校服务器地址。可通过查询获知，例如 vpn.sydney.edu.au</li><li>Username, 即学生 Unikey</li><li>Password, 即与Unikey 所对应的密码</li></ol><p>Windows 及 macOS 客户端下载网址可正常访问：vpn.sydney.edu.au</p><p>移动端下载地址：<a href="https://apps.apple.com/us/app/cisco-anyconnect/id1135064690">iOS</a>, <a href="https://play.google.com/store/apps/details?id=com.cisco.anyconnect.vpn.android.avf&hl=en_US">Google Play</a>, <a href="https://c-t.work/s/e5d7615075214f">APK下载</a></p><p>学校提供的 VPN 服务器速度有限，其重点在于提高访问的稳定性，减少因防火墙造成的网络丢包现象进而提升访问效率。如果需要进一步的提升速度以满足高质量音视频需求，可使用其他<a href="https://github.com/bigdongdongCLUB/newGCP/issues/1">教程</a>。需要注意的是，此种模式下设备的全部网络流量都会过代理服务器，因此原服务位于国内的网络资源，如微信与微博，会绕道海外服务器，增加访问服务的响应时间。</p><p>如有其他问题，可直接联系我。</p>]]></content>
      
      
      
        <tags>
            
            <tag> Tool </tag>
            
            <tag> Study </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>双拼使用体验：打字效率翻倍？</title>
      <link href="%E5%8F%8C%E6%8B%BC%E4%BD%BF%E7%94%A8%E4%BD%93%E9%AA%8C%EF%BC%9A%E6%89%93%E5%AD%97%E6%95%88%E7%8E%87%E7%BF%BB%E5%80%8D%EF%BC%9F/"/>
      <url>%E5%8F%8C%E6%8B%BC%E4%BD%BF%E7%94%A8%E4%BD%93%E9%AA%8C%EF%BC%9A%E6%89%93%E5%AD%97%E6%95%88%E7%8E%87%E7%BF%BB%E5%80%8D%EF%BC%9F/</url>
      
        <content type="html"><![CDATA[<p>大概从 2019 年 2 月份开始，我开始学习使用小鹤双拼，并从记忆键位表开始就完全关掉了全拼输入法，强制让自己去练习使用双拼。实际证明这样的方法非常有效，到 5 月份以后，我已经能够使用双拼以原来全拼的速度打字了。这件事情最困难的点在于，手上的行动会完全跟不上自己的思维，因此在需要极速打字的场景，例如同时对多人聊天的场景下，就会出现急得跳脚的情况。至于打字效率有没有翻倍，至少我现在还没有。但我认为实现只是时间问题，现在要做的是把现有的肌肉记忆进一步固化进潜意识了，如果做到凭直觉去打字那么效率就会高的多。</p><span id="more"></span><p>那么还是简单介绍一下什么是双拼输入法吧。总的来说，中文输入法分为字音输入和字形输入。非常古典的五笔输入法就是字形输入，通过不同按键对应不同的偏旁或笔划去检索要输入的汉字。字形输入法的学习门槛较高，因为需要记忆每个笔画对应着的不同按键，但由于没有重音字的问题，其输入效率非常高。另一方面，大家都熟悉的全拼输入法，顾名思义，就是把一个拼音的全部字母敲到键盘上，例如「上学去」三个字就需要「s, h, a, n, g, x, u, e, q, u」一共 10 个字母。那么双拼输入法怎样输入呢？以小鹤双拼为例，「sh」对应英文键盘的「u」,「ang」对应于英文键盘的「h」而「ue」对应「t」。也就是说只要输入「u, h, x, t, q, u」就可以打出「上学去」这三个字。从例子中我们可以看出，双拼输入法就是将多个英文字母的拼音字母，如「sh, ong, ui」等分别映射到不同的按键上，就可以实现无论是什么样拼音的单字都能通过键盘上的两个按键输入进去。并且在双拼熟练之后，每两次按键就输入单字会形成一种韵律感，从而大大提高输入效率。</p><p>双拼输入法有很多种，其差别只是不同按键对应的拼音字母不同罢了。至于用哪种，我只是用过小鹤双拼，其键位布局较为合理使用也比较广泛，在 iOS 和 macOS 中都有已经预装。另一种使用广泛的是微软双拼，只要是 Windows 系统都附带有微软双拼，但有一个槽点是其将分号键也映射为拼音导致其在触摸虚拟键盘上的字母键位不同于英文键盘。</p>]]></content>
      
      
      
        <tags>
            
            <tag> Life </tag>
            
            <tag> Productivity </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Summaries of a book —— Make Your Own Neural Network</title>
      <link href="Summaries%20of%20a%20book%20%E2%80%94%E2%80%94%20Make%20Your%20Own%20Neural%20Network/"/>
      <url>Summaries%20of%20a%20book%20%E2%80%94%E2%80%94%20Make%20Your%20Own%20Neural%20Network/</url>
      
        <content type="html"><![CDATA[<h2 id="Prologue"><a href="#Prologue" class="headerlink" title="Prologue"></a>Prologue</h2><p>This is my first time to write a blog in English and so I would like to explain reasons which encourage me to do this. Briefly, it can help me learn. First, because I am a Chinese native speaker, I can write slower and have more time to acquire and understand the knowledge in the book totally. Second, codes and functions are written by English in this book. Thus, thinking in the same language will help me avoid issues of translation, especially the special words in codes. Last but not least, the process can be credit to improve my English skills which is very superficial.</p><span id="more"></span><p><em>Make Your Own Neural Network</em> is a basic introduced book for people who hope to know mechanisms of neural network and machine learning. Even if people don’t want to learn programming, the simple explanation can teach them how a computer simulates neural cells to work like the human’s brain. There are the book’s links at <a href="https://book.douban.com/subject/26945232/">Douban</a>, <a href="https://www.amazon.com/Make-Your-Own-Neural-Network/dp/1530826608/ref=sr_1_2?ie=UTF8&qid=1547821621&sr=8-2&keywords=make+your+own+neural+network">Amazon</a> and my <a href="https://raw.githubusercontent.com/jakewg/jw_blog_file_host/master/Make%20Your%20Own%20Neural%20Network/Make%20Your%20Own%20Neural%20Network_withMarginNotes.pdf"> mindnote</a>, <a href="https://raw.githubusercontent.com/jakewg/jw_blog_file_host/master/Make%20Your%20Own%20Neural%20Network/neural%20network%20backup.py">final code backup</a> at Github.</p><h2 id="Mechanism"><a href="#Mechanism" class="headerlink" title="Mechanism"></a>Mechanism</h2><h3 id="A-new-age-inspired-by-a-nature"><a href="#A-new-age-inspired-by-a-nature" class="headerlink" title="A new age inspired by a nature"></a>A new age inspired by a nature</h3><p>After the computer having popularized, many boring and tough tasks are no longer difficult for people, such as calculating millions of numbers. Although computers can deal with complex multi-problems simultaneously, they have no ability to identify is there a cat or a baby in a photo. Therefore, people say why not try to build artificial brains by copying how real biological brains worked? The next step is that using programs to make machines cope with tasks just like what human neural cells do.</p><h3 id="A-simple-predicting-machine"><a href="#A-simple-predicting-machine" class="headerlink" title="A simple predicting machine"></a>A simple predicting machine</h3><p>Like all computer systems, we should have an input and an output with some calculation in between. Compared with math solutions, we may do not know how the process happen. So we could only use a model with parameters which we can adjust to represent this mysterious process. After that, a good method to refine these models is to adjust the parameters based on how wrong the model is compared to know true examples.</p><h3 id="Training-a-simple-classifier"><a href="#Training-a-simple-classifier" class="headerlink" title="Training a simple classifier"></a>Training a simple classifier</h3><p>Training data means that examples of truth used to teach a predictor or a classifier.</p><p><img src="https://raw.githubusercontent.com/jakewg/jw_blog_file_host/master/pages_img/make_you_own_neural_network_0.jpg"></p><p>Through these graphs, we can know how to use the error to inform how we adjust the slope. We can use the equation <em>∆A = E / x</em> to update the original A many times and simply match the last training example closely. Besides, we need use <strong>learning rate</strong> to moderate the updates which is helpful to avoid errors and noises from the real world. </p><p>But one classifier is not enough.  It cannot separate data where that data itself isn’t governed by a single linear process. Boolean functions, <strong>AND, OR, XOR</strong> are also very necessary when we just use multiple linear classifiers to divide up data.</p><h3 id="Neurons-nature’s-computing-machines"><a href="#Neurons-nature’s-computing-machines" class="headerlink" title="Neurons, nature’s computing machines"></a>Neurons, nature’s computing machines</h3><p>Neurons all transmit an electrical signal from one end to the other, from dendrites along the axons to the terminals. It takes an electric input, and pops out another electrical signal. They do not react readily but instead suppress the input until it has grown so large that it triggers and output. We can use the logistic function to simulate the process’s characteristic.</p><p><img src="https://raw.githubusercontent.com/jakewg/jw_blog_file_host/master/pages_img/make_you_own_neural_network_1.jpg"></p><p>One way to replicate this from nature to an artificial model is to have layers of neurons, with each connected to every other one in the preceding and subsequent layer. By this method, our computer model will also attain the ability to be incredibly resilient to damage and imperfect signals just like human brains. To make the model learn, we should adjust the strength of connections between nodes. Therefore, the mechanism ought to de-emphasize a weight which lead to the wrong answer and raise the weight which lead to the right answer.</p><p><img src="https://raw.githubusercontent.com/jakewg/jw_blog_file_host/master/pages_img/make_you_own_neural_network_2.jpg"></p><h3 id="Matrix-multiplication-is-useful"><a href="#Matrix-multiplication-is-useful" class="headerlink" title="Matrix multiplication is useful"></a>Matrix multiplication is useful</h3><p>First, matrix allow us to compress writing all those calculations into a very simple short form. Second, many computer programming languages understand working with matrices quickly and efficiently.</p><p><em>X = W·I</em> and <em>O = sigmoid (X)</em> W is the matrix of weight, I is the matrix of inputs.</p><p><img src="https://raw.githubusercontent.com/jakewg/jw_blog_file_host/master/pages_img/make_you_own_neural_network_3.jpg"></p><h3 id="Learning-weights-from-more-than-one-node"><a href="#Learning-weights-from-more-than-one-node" class="headerlink" title="Learning weights from more than one node"></a>Learning weights from more than one node</h3><p>The next step is to use the output from the neural network and compare it with the training example to work out an error. We used the error, the difference between what the node produced as an answer and what we know the answer should, to guide that refinement. So we use the proportion of weights to split the output error. In below graphs, the error e1 is split in proportion to the connected links, which have weight w11 and w21.</p><p><img src="https://raw.githubusercontent.com/jakewg/jw_blog_file_host/master/pages_img/make_you_own_neural_network_4.jpg"></p><p>If we had even more layers, we’d repeatedly apply this same idea to each layer working backwards from the final output layer.</p><h3 id="How-do-we-actually-update-weights"><a href="#How-do-we-actually-update-weights" class="headerlink" title="How do we actually update weights"></a>How do we actually update weights</h3><p>The mathematical expressions showing how all the weights result in a neural network’s output are too complex to easily untangle. Thus, we can use gradient descent method to find the minimum without actually having to understand functions. It is very necessary for us to refine without the size of the steps overshooting the minimum and forever bouncing around it.</p><p><img src="https://raw.githubusercontent.com/jakewg/jw_blog_file_host/master/pages_img/make_you_own_neural_network_5.jpg"></p><p>To avoid ending up in the wrong valley, or function minimum, we train neural networks several times starting from different points on the hill to ensure we don’t always ending up in the wrong valley. So this method is also resilient to imperfections in the data, we don’t go wildly wrong if the function isn’t quite perfectly. </p><p>Here’s the final answer we’ve been working towards, the one that describes the slope of the error function so we can adjust the weight wjk. And we also use 𝛂 as a learning rate</p><p><img src="https://raw.githubusercontent.com/jakewg/jw_blog_file_host/master/pages_img/make_you_own_neural_network_6.jpg"></p><h3 id="Preparing-date"><a href="#Preparing-date" class="headerlink" title="Preparing date"></a>Preparing date</h3><h4 id="input"><a href="#input" class="headerlink" title="input"></a>input</h4><p>A very flat activation function is problematic because we use the gradient to learn new weights. A tiny gradient means we’ve limited the ability to learn which called <strong>saturating a neural network</strong>. Besides, a very very tiny values can be problematic too because computers can lose accuracy when dealing very very small or very very large numbers. The good range of input is range 0.0 to 1.0. To avoid killing the learning ability by zeroing the weight update, zero inputs should not be included in.</p><h4 id="output"><a href="#output" class="headerlink" title="output"></a>output</h4><p>The logistic function doesn’t get to 1.0, and it just gets closer to it which is called this asymptotically approaching 1.0. If we do set target values in the inaccessible ranges, the training will drive ever large weights in an attempt to produce larger and larger output.</p><h3 id="random-initial-weights"><a href="#random-initial-weights" class="headerlink" title="random initial weights"></a>random initial weights</h3><p>We should choose initial weights randomly and uniformly from range -1.0 to +1.0. and avoid large initial weights because they cause large signals into an activation function and undermine the effort we put into carefully scaling the input signals. Moreover, we don’t set the initial weights the same constant value, especially. Obviously, zero weights can kill the input signal and error back propagating should not be divided equally.</p><h2 id="DIY-with-Python"><a href="#DIY-with-Python" class="headerlink" title="DIY with Python"></a>DIY with Python</h2><h3 id="A-very-gentle-start-with-Python"><a href="#A-very-gentle-start-with-Python" class="headerlink" title="A very gentle start with Python"></a>A very gentle start with Python</h3><p>Class: we have created an object called sizzles from the Dog class definition and we can consider this object to be a dog.  </p><pre><code>sizzles =  Dog() sizzles.bark()#initialisation method with internal datadef __init__(self, petname, temp):self.name = petname;self.temperaturn = temp:pass</code></pre><p>The <code>self.</code> part means that the variables are part of this object and are independent of another Dog object or general variables in Python.</p><h3 id="Neural-network-with-Python"><a href="#Neural-network-with-Python" class="headerlink" title="Neural network with Python"></a>Neural network with Python</h3><p>The skeleton code:</p><ul><li>initialization: to set the number of input, hidden and output nodes.</li><li>train: refine the weights after being given a training set example to learn from.</li><li>query: give an answer from the output nodes after being given an input.</li></ul><p>The most important part of the network is the link weights expressed as a matrix. <em>Xhidden = Winput_hiddenx · I</em> <em>Ohidden = sigmoid · ( Xhidden )</em></p><p>Training the network:</p><ul><li>working out the output for a given training example. That is no different to what we just did with the query() function.</li><li>taking this calculated output, comparing it with the desired output, and using the difference to guide the updating of the network weights.</li></ul><h3 id="The-NMIST-dataset-of-hand-written-numbers"><a href="#The-NMIST-dataset-of-hand-written-numbers" class="headerlink" title="The NMIST dataset of hand written numbers"></a>The NMIST dataset of hand written numbers</h3><p>There is my <a href="https://raw.githubusercontent.com/jakewg/jw_blog_file_host/master/Make%20Your%20Own%20Neural%20Network/neural%20network%20backup.py">final codes backup</a> on GIthub.</p><p>In the <em>.cvs</em> file, the first value is the label which is the actual digit that the handwriting is supposed to represent. The subsequent values means the original picture’s pixel values. We did see earlier how we might plot a rectangular array of numbers using the <code>imshow()</code> function. We want to do the same here but we need to convert that list of comma separated numbers into a suitable array. Split that long text string of comma separated values into individual values, using the commas as the place to do the splitting. Ignore the first value, which is the label, and take the remaining list of 28 x 28 = 784 values and turn them into an array which has a shape of 28 rows by 28 columns and plot the array.</p><p>We’ve deliberately chosen 0.01 as the lower end of the range to avoid the problems we saw earlier with zero valued inputs because they can artificially kill weight updates. We don’t have to choose 0.99 for the upper end of the input because we don’t need to avoid 1.0 for the inputs. It’s only for the outputs that we should avoid the impossible to reach 1.0.</p><p>By choosing a value smaller than the number of inputs, we force the network to try to summaries the key features. But if we choose too few hidden layer nodes, then we restrict the ability of the network to find sufficient features or patterns.</p><p>Finally, we can do some improvements.</p><ul><li>The first improvement we can try is to adjust the learning rate. We set it at 0.3 previously without really experimenting with different values.</li><li>The next improvement we can do is to repeat the training several times against the data set. Some people call each run through an epoch.</li><li>Another idea is that the learning rate is too high for larger numbers of epochs.</li><li>Try changing the number of middle hidden layer nodes.</li></ul><h2 id="Even-More-Fun"><a href="#Even-More-Fun" class="headerlink" title="Even More Fun"></a>Even More Fun</h2><h3 id="Inside-the-mind-of-a-neural-network"><a href="#Inside-the-mind-of-a-neural-network" class="headerlink" title="Inside the mind of a neural network"></a>Inside the mind of a neural network</h3><p>Once a neural network is trained, and performs well enough on test data, we essentially have a mysterious black box. We don’t really know how it works out the answer - it just does.</p><h3 id="Creating-new-training-data-rotations"><a href="#Creating-new-training-data-rotations" class="headerlink" title="Creating new training data: rotations"></a>Creating new training data: rotations</h3><p>we could create new ones from those by rotating them clockwise and anti-clockwise, by 10 degrees for example. o by adding training examples with overly rotated images, we are reducing the quality of the training by adding false examples. Ten degrees seems to be the optimal angle for maximizing the value of additional data.</p>]]></content>
      
      
      
        <tags>
            
            <tag> Study </tag>
            
            <tag> Books </tag>
            
        </tags>
      
    </entry>
    
    
  
  
    
    
    <entry>
      <title>tags</title>
      <link href="tags/index.html"/>
      <url>tags/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="about/index.html"/>
      <url>about/index.html</url>
      
        <content type="html"><![CDATA[<center>About me</center>---<p>Don’t panic &amp; Do more.<br>You can contact me on <a href="https://github.com/jakewg">github</a></p>]]></content>
      
    </entry>
    
    
  
</search>
